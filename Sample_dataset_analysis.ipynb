{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dear sir \\nGood day to you. We are Wholesale manufacturer and exporter in customized soccer uniform,baseball uniform and sports wear and undergarments etc.\\n\\n\\n1. We are expert team of managers, merchandisers, designers and most of all great professional stitching labour.\\n\\n2. fast turnaround time.\\n\\n3. artworks within 12-24hours.\\n\\n4. warranty on our made goods\\n\\n5. unbeatable quality and unbeatable prices\\n\\n6. wide range of customization\\n\\n7. we offer free samples\\n\\nFeel free to tell us if you need any customized artwork or customized sample.\\n\\nHoping to hear from you soon on my offer!\\n\\nThank you so much!\\n\\nasquareindustry524@gmail.com\\n\\nRegards:\\nAsquareindustry \\nAlso contact us on WhatsApp: +923321208372']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# File path for JSON dataset\n",
    "file_path = 'data.json'\n",
    "\n",
    "# Load JSON data\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extracting the 'text' content from the dataset\n",
    "texts = [item['_source']['text'] for item in data] if isinstance(data, list) else [data['_source']['text']]\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/homebrew/lib/python3.10/site-packages (4.3.2)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/15/fc/7b6dd7e1adc0a6407b845ed4be1999e98b6917d0694e57316d140cc85484/transformers-4.39.3-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentence-transformers\n",
      "  Obtaining dependency information for sentence-transformers from https://files.pythonhosted.org/packages/ba/20/7ef81df2e07322d95332d07c1c38c597f543c1f666d689a3153ba6fa09e3/sentence_transformers-2.6.1-py3-none-any.whl.metadata\n",
      "  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/homebrew/lib/python3.10/site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/homebrew/lib/python3.10/site-packages (from gensim) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/homebrew/lib/python3.10/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from transformers) (3.12.3)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/05/c0/779afbad8e75565c09ffa24a88b5dd7e293c92b74eb09df6435fc58ac986/huggingface_hub-0.22.2-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/danicaliang/Library/Python/3.10/lib/python/site-packages (from transformers) (23.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/5b/07/10033a403b23405a8fc48975444463d3d10a5c2736b7eb2550b07b367429/PyYAML-6.0.1-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/01/04/45d88b8bddc09bf56ae1631721393255b75798af515c65c26389713a2072/tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/a8/50/d21bb6189f7996a1819a9e3a656689b69d416c393aecb2a759569e8eb7de/safetensors-0.4.2-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers) (10.0.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/93/6d/66d48b03460768f523da62a57a7e14e5e95fdf339d79e996ce3cecda2cdb/fsspec-2024.3.1-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "Downloading PyYAML-6.0.1-cp310-cp310-macosx_11_0_arm64.whl (169 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.3/169.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-macosx_11_0_arm64.whl (393 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.4/393.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Installing collected packages: safetensors, pyyaml, fsspec, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed fsspec-2024.3.1 huggingface-hub-0.22.2 pyyaml-6.0.1 safetensors-0.4.2 sentence-transformers-2.6.1 tokenizers-0.15.2 transformers-4.39.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install gensim transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dear', 'sir', 'good', 'day', 'wholesale', 'manufacturer', 'exporter', 'customized', 'soccer', 'uniform', 'baseball', 'uniform', 'sports', 'wear', 'undergarments', 'etc', 'expert', 'team', 'managers', 'merchandisers', 'designers', 'great', 'professional', 'stitching', 'labour', 'fast', 'turnaround', 'time', 'artworks', 'within', 'warranty', 'made', 'goods', 'unbeatable', 'quality', 'unbeatable', 'prices', 'wide', 'range', 'customization', 'offer', 'free', 'samples', 'feel', 'free', 'tell', 'us', 'need', 'customized', 'artwork', 'customized', 'sample', 'hoping', 'hear', 'soon', 'offer', 'thank', 'much', 'gmailcom', 'regards', 'asquareindustry', 'also', 'contact', 'us', 'whatsapp']]\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization: Split the text into words/tokens.\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing: Convert all characters to lowercase to ensure uniformity.\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Removing Punctuation: Strip punctuation from each word.\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # Removing Non-Alphabetic Characters: Filter out any tokens that are not alphabetic.\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # Stopwords Removal: Filter out stopwords like 'the', 'is', etc.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    # The result is a list of clean, lowercased, alphabetic tokens without any stopwords.\n",
    "    return words\n",
    "\n",
    "\n",
    "# Apply preprocessing to each text\n",
    "processed_data = [preprocess_text(text) for text in texts]\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=58, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Creating the Word2Vec model\n",
    "model = Word2Vec(processed_data, vector_size=100, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.3844e-02,  1.0504e-01,  4.0992e-02, -4.2579e-02, -2.3354e-01,\n",
      "         -3.9933e-01,  1.1702e-01,  4.6059e-01, -7.6370e-02, -1.8849e-01,\n",
      "          1.2808e-01,  3.3672e-02,  1.8109e-01,  8.5573e-02,  4.0654e-02,\n",
      "          1.6405e-01, -4.2237e-01,  5.8249e-01,  2.5443e-01,  3.0992e-01,\n",
      "         -2.7429e-01, -9.7950e-01,  2.1268e-01, -3.9676e-01,  1.7533e-03,\n",
      "         -4.0917e-01, -3.8727e-02, -3.5095e-01, -7.8682e-02,  1.0377e-01,\n",
      "          3.8977e-01,  3.3523e-01, -3.9679e-02, -5.1565e-01,  3.6672e-01,\n",
      "         -3.2922e-01,  3.8769e-01, -1.5823e-01,  2.9197e-01,  1.5891e-01,\n",
      "         -1.4887e-02,  2.9068e-01,  6.2196e-01,  1.5573e-01, -6.9632e-02,\n",
      "         -1.0264e-01, -3.0597e+00, -1.4376e-01, -4.2515e-02, -3.5150e-01,\n",
      "          5.5508e-01, -2.2956e-01,  2.7049e-01,  1.6017e-01,  2.8476e-01,\n",
      "          5.1459e-01, -6.0721e-01,  2.0733e-01, -9.8158e-02,  4.4031e-01,\n",
      "         -1.0629e-02,  1.9578e-02, -2.8376e-01,  9.5047e-03,  2.3789e-01,\n",
      "          3.4480e-01, -1.6752e-01,  5.0765e-01, -3.6599e-01,  6.6591e-01,\n",
      "          4.2414e-02,  6.3859e-02,  3.6905e-01, -1.7440e-01,  1.6710e-01,\n",
      "         -1.0290e-01, -6.9567e-02,  1.5158e-01, -1.6005e-01, -1.6415e-01,\n",
      "          2.8554e-03,  4.9895e-01,  2.3354e-01, -2.6239e-01, -5.9325e-02,\n",
      "          5.5551e-01,  1.9174e-01, -2.4462e-01,  3.3676e-01,  3.6802e-01,\n",
      "          1.1619e-01, -2.4375e-01,  2.4250e-01,  5.2859e-02,  6.9732e-01,\n",
      "         -1.1412e-01,  1.6800e-01,  1.5526e-01,  2.5515e-02,  3.1757e-01,\n",
      "          6.3006e-02,  1.5664e-01,  2.0175e-01, -3.6927e-01,  2.6239e-01,\n",
      "          4.6862e-02,  1.1547e-01, -1.3627e-01,  5.5669e-01, -2.4704e+00,\n",
      "          4.8897e-01,  6.0506e-01, -1.9987e-01, -2.2122e-01, -4.0023e-01,\n",
      "         -1.7305e-01,  6.0682e-01, -1.4574e-01, -1.9003e-01,  1.6980e-01,\n",
      "         -3.2671e-01,  8.4918e-01,  1.3678e-01, -1.8561e-01,  2.1390e-01,\n",
      "          3.4918e-01,  9.9168e-02, -1.7788e-01,  5.4497e-01,  2.1170e-01,\n",
      "          1.4739e-01,  2.6614e-01, -2.4237e-01, -2.2281e-01, -2.3935e-01,\n",
      "          1.4548e-01,  3.4895e-01, -2.9137e-01, -2.9297e-01, -3.6769e-02,\n",
      "         -4.1963e-01, -1.2812e-01, -2.9716e+00,  2.3097e-01,  4.8550e-01,\n",
      "          4.0997e-02, -2.6022e-01,  1.0485e-01, -3.1962e-01,  1.2483e-01,\n",
      "          8.1745e-03, -2.8756e-01, -1.3999e-01,  2.5642e-01, -4.8736e-01,\n",
      "          2.2094e-01, -1.7983e-01, -5.5665e-01,  3.8769e-02,  2.5611e-01,\n",
      "          1.0601e-01,  3.2140e-01,  7.7694e-02, -2.9631e-01, -1.0394e-01,\n",
      "          2.1300e-01, -5.0715e-03,  3.0181e-01, -2.8666e-02, -1.4478e-02,\n",
      "         -1.6171e-01,  2.7745e-01,  6.5542e-01, -1.4899e-01,  6.6678e-01,\n",
      "         -7.2367e-02,  5.4488e-02,  3.8504e-01,  3.6753e-02,  2.9226e-01,\n",
      "         -3.3843e-01,  5.9267e-02, -4.0321e-02,  2.7090e-01,  1.9891e-01,\n",
      "         -3.4924e-01,  4.9598e-01,  4.1543e-01, -9.4758e-02,  3.0834e-02,\n",
      "         -9.7146e-03, -1.6591e-01,  3.5552e-01,  4.8640e-01,  7.5211e-01,\n",
      "          2.9251e-01,  1.1861e-01, -1.3358e-01,  3.9743e-01,  1.1171e-01,\n",
      "          3.4082e-01,  7.0096e-02,  1.2332e-02,  1.6913e-01, -7.3704e-01,\n",
      "          3.6691e+00,  1.4218e-01, -2.3193e-01,  2.7710e-01,  3.3914e-01,\n",
      "          2.3274e-01,  6.5044e-02, -2.7059e-01, -4.3812e-01, -3.7369e-02,\n",
      "         -1.3421e-01,  2.6963e-01, -3.0980e-02, -3.4776e-01, -3.0440e-02,\n",
      "          6.0294e-02,  6.3313e-01,  7.9843e-02,  3.6142e-01, -7.5955e-02,\n",
      "          4.0224e-01, -1.6062e-01, -3.5818e-01,  1.1426e-01, -1.2640e+00,\n",
      "          2.9549e-01, -3.5285e-01, -6.4344e-01,  1.2360e-01, -1.1363e-01,\n",
      "          1.9179e-02,  1.0443e-01, -2.7997e-01,  3.7258e-01,  3.2824e-01,\n",
      "          1.2953e-01,  3.3352e-01, -3.9616e-01,  1.9217e-01, -5.5978e-02,\n",
      "          8.0023e-02,  3.8030e-02, -5.0352e-01,  2.8965e-01,  2.2333e-02,\n",
      "          5.7569e-01, -1.2439e-01,  2.2713e-01, -4.6766e-02,  4.0425e-01,\n",
      "          1.1803e-01,  3.4289e-02, -7.0126e-02, -2.6899e-01, -1.1758e-01,\n",
      "         -3.3286e-01,  8.7609e-02,  1.6015e-01, -1.0271e-01, -3.8219e-01,\n",
      "         -9.1660e-01, -1.9392e-01, -4.8626e-01, -6.9925e-02, -1.4763e-01,\n",
      "          2.0653e-01,  1.0372e-02, -4.3152e-01, -3.5753e+00,  4.9563e-01,\n",
      "          1.9727e-01,  1.9716e-01,  5.5648e-01,  2.7339e-01, -4.2592e-01,\n",
      "          1.9678e-01,  3.1821e-01, -9.5254e-02,  3.8295e-01,  2.1827e-02,\n",
      "         -3.9373e-01, -2.0934e-01, -9.4868e-02,  1.8946e-01,  2.8846e-01,\n",
      "          1.0532e-01, -2.0955e-01, -2.4244e-02,  4.0030e-02,  2.8928e-01,\n",
      "         -2.5201e-01,  6.1657e-02, -1.3672e-01, -3.1027e-02, -3.4302e-01,\n",
      "         -5.4900e-01,  6.2076e-01, -3.4925e-01,  1.0764e-01,  7.8710e-02,\n",
      "          8.4808e-02, -3.1195e-01, -6.5562e-01, -3.3959e+00, -3.0116e-03,\n",
      "          4.4142e-02, -3.3558e-01, -3.9240e-01, -2.0391e-01,  4.7001e-01,\n",
      "          5.3472e-02,  9.0435e-02,  3.7281e-02,  1.0725e-01,  2.8654e-01,\n",
      "          1.1817e-01,  3.4399e-02,  1.2063e-01,  8.1058e-01,  1.2533e-01,\n",
      "          3.9422e-01,  2.1661e-01,  9.2442e-02, -1.3886e-01,  1.6823e-01,\n",
      "          1.0202e-01,  2.1383e-01,  1.8635e-01,  2.5943e-01, -3.6704e-01,\n",
      "         -2.0538e-01, -6.8800e-02, -3.0769e-02,  5.5560e-01, -3.3032e-03,\n",
      "         -2.0001e-01,  3.4346e-03, -4.0630e-01, -4.2442e-01,  6.7622e-02,\n",
      "          6.0797e-02,  3.0178e-01, -1.1362e-01, -2.1951e-01,  7.6077e-01,\n",
      "         -5.8365e-02,  1.3364e-01,  5.0073e-01, -2.0693e-01, -1.6660e-01,\n",
      "         -2.9109e-02,  2.6608e-01,  2.4837e-01,  1.9685e-01, -3.3650e-01,\n",
      "          1.1813e+00, -2.1417e-01,  5.1965e-03, -4.4422e-02,  4.5540e-01,\n",
      "          2.8781e-01,  3.4638e-01,  8.4210e-02,  5.9813e-01, -8.4038e-02,\n",
      "          6.9699e-02, -1.1006e-01,  3.9708e-02, -2.1923e-01,  3.6864e-01,\n",
      "         -4.5525e-01, -2.1407e-01,  1.1851e-01, -3.5917e-01,  7.3806e-01,\n",
      "         -1.7912e-01, -8.2481e-01,  1.9657e-02,  1.5132e-01,  1.3666e-01,\n",
      "          5.6528e-01,  4.0992e-01,  2.5251e-01, -2.6340e-01, -1.5774e-01,\n",
      "         -1.4184e-01,  3.8209e-01, -4.8870e-01,  5.3838e-03, -2.0091e-01,\n",
      "          2.8239e-01, -8.2486e-01,  1.8098e-01, -5.4563e-01,  5.3091e-01,\n",
      "         -5.0096e-03, -7.2252e-02,  2.1838e-01, -9.4190e-02,  7.3754e-01,\n",
      "         -7.9842e-01,  1.4495e-01, -2.7429e-01, -1.1297e-01, -1.3305e-01,\n",
      "         -4.2799e-02, -4.1879e-01, -8.0486e-03,  9.0667e-02, -3.1257e-01,\n",
      "          6.8925e-02,  1.3287e-01,  7.2070e-02, -4.1598e-01,  6.6912e-02,\n",
      "         -5.5702e-01,  3.3109e-01,  7.1768e-01, -9.5096e-02,  1.9284e-01,\n",
      "          4.4991e-01,  3.9966e-01,  2.3473e-01,  1.8837e-01,  5.3267e-02,\n",
      "          5.7724e-02, -5.3133e-02, -3.2652e-01, -4.2943e-03,  1.5638e-02,\n",
      "         -2.5378e-01, -2.1258e-01, -4.4084e-02, -1.4842e-01, -1.7129e-01,\n",
      "         -2.2856e-01, -4.3807e-01,  7.7381e-02,  3.9704e-02, -1.9483e-02,\n",
      "          2.1780e-01,  1.2855e-01, -4.0774e-01,  3.3751e-01, -1.8220e-01,\n",
      "         -2.6813e-01,  4.6251e-01, -1.0708e-01,  3.7030e-01,  3.0575e-01,\n",
      "         -1.3834e-02, -3.5263e-01,  2.2827e-01, -4.3386e-01, -3.4572e-01,\n",
      "          1.5176e-01, -3.9790e-01,  3.9190e-01, -9.8951e-02, -3.5409e-01,\n",
      "         -2.3866e-01, -1.3853e-01, -6.6746e-01,  2.6804e-02,  4.3832e-01,\n",
      "         -1.5006e+00,  2.1261e-01,  4.7246e-01, -3.0541e-02,  1.1613e-01,\n",
      "         -5.6064e-01, -2.4404e-01,  3.1741e-01, -5.1111e-02,  2.4286e-01,\n",
      "         -2.1031e-01, -5.3100e-01, -5.2796e-02,  2.1217e-01,  3.3087e-01,\n",
      "         -3.8378e-02,  1.8637e-01, -1.7559e-01, -8.8509e-02,  3.1353e-02,\n",
      "          4.9529e-02,  7.1039e-01,  6.1565e-02, -1.1301e-01,  2.0065e-01,\n",
      "         -1.9010e-01, -2.9972e-01,  5.3995e-01, -3.9690e-01,  3.3468e-01,\n",
      "          1.4716e-01, -5.4148e-01, -5.1219e-01,  1.3378e-01,  4.1882e-01,\n",
      "          2.7252e-01,  1.5910e-01, -4.0703e-01,  3.8193e-01,  2.6648e-01,\n",
      "         -6.2429e-01,  4.5643e-01,  1.4035e-01,  3.1058e-01,  1.7675e-01,\n",
      "          2.2043e-01, -5.9958e-01,  3.1167e-02,  3.6252e-01, -5.8856e-02,\n",
      "          9.8659e-02,  2.7455e-01, -1.9092e-01, -4.8379e-01, -4.4847e-02,\n",
      "          3.8339e-01,  3.0255e-01,  2.2259e-01, -1.8411e-01,  2.0150e-02,\n",
      "          3.6651e-02, -1.7258e-01, -1.2219e-01,  1.7771e-01, -2.8994e-01,\n",
      "         -6.5179e-01,  5.9864e-02,  1.0514e-01, -1.0413e-01,  2.1736e-01,\n",
      "          1.3766e-01,  1.5767e-01, -5.8639e-01,  5.2968e-01, -1.9059e-01,\n",
      "         -1.3522e-02,  1.0886e-01, -1.5225e-01,  1.3015e-01, -5.8448e-01,\n",
      "          7.2456e-02, -6.5469e-01, -3.7295e-01,  5.1290e-01, -4.5631e-01,\n",
      "          3.2475e-01, -2.8444e-01, -2.1264e-02,  1.9251e-01,  8.1486e-02,\n",
      "         -4.8760e-01, -5.5075e-01,  5.7101e-02,  3.8471e-01, -3.5125e-02,\n",
      "          1.6076e-01, -3.3043e-01,  1.7434e-01,  2.1130e-01,  2.4066e-02,\n",
      "         -1.4062e-02,  4.3066e-01,  3.4127e-01,  2.6295e-01,  2.9872e-01,\n",
      "         -1.1106e-01,  2.3102e-01, -1.8450e-01, -3.7214e-01,  5.4202e-01,\n",
      "          8.6962e-02,  6.0926e-02, -3.7902e-01, -2.3007e-01,  9.9424e-02,\n",
      "         -4.3587e-01,  7.2345e-02,  7.1967e-02,  2.1073e+00, -3.3599e-02,\n",
      "         -2.1480e-02,  1.2312e-01,  4.4728e-01,  1.3834e-01, -4.2322e-01,\n",
      "          2.8845e-02, -2.5083e-01,  2.9634e-01, -1.2289e-01,  2.4056e-01,\n",
      "         -1.5332e-01,  3.9218e-02,  4.6280e-01,  2.7201e-01,  9.3820e-02,\n",
      "         -8.0360e-01, -3.6522e-02, -2.7679e-01, -5.3203e-01,  3.1636e-01,\n",
      "          3.8826e-01,  7.7096e-02,  1.4795e-01,  3.6247e-01, -3.4087e-01,\n",
      "         -2.6574e-01,  4.6313e-01, -1.7424e-02, -4.6917e-01, -1.6123e-01,\n",
      "          2.3049e-01,  7.4066e-01, -2.7196e-01,  3.2367e-01, -3.3651e-03,\n",
      "         -6.6625e-01, -1.8895e-01,  1.3051e-02, -1.6503e-01, -2.6358e-01,\n",
      "          6.4059e-01, -5.7076e-02, -1.9698e-01,  4.6252e-01, -1.1269e-01,\n",
      "         -5.5268e-01,  3.7492e-01, -6.3664e-02, -7.6375e-02, -1.6065e-01,\n",
      "         -1.8069e-01,  1.6369e-01, -1.3180e-01, -1.8845e-01, -2.9541e-01,\n",
      "         -3.5792e-01, -3.5901e-01,  3.8354e-01, -2.4992e-01,  1.1226e-01,\n",
      "          3.4836e-01, -4.4226e-01,  2.7302e-01, -1.7305e-01, -2.5866e-01,\n",
      "          2.9240e-01,  2.7755e-01, -3.0014e-01, -2.1532e-01,  2.0701e-01,\n",
      "          6.6422e-02,  3.1455e-01,  1.6755e-01, -1.2028e-01,  4.0634e-01,\n",
      "          9.1291e-03,  6.0916e-02, -2.8352e+00,  4.4308e-01,  3.5193e-01,\n",
      "          2.2523e-01, -8.8997e-02,  1.7427e-01, -7.4207e-03,  2.0700e-01,\n",
      "         -1.3000e-02,  9.4382e-03,  8.3418e-02,  1.1547e-01, -2.1516e-02,\n",
      "         -2.6730e-01, -2.9307e-03, -3.0535e-02, -1.9772e-02,  6.4368e-03,\n",
      "         -5.5927e-01,  1.0100e-01,  3.8562e-01,  4.6792e-01, -3.0451e-01,\n",
      "         -2.8955e-01, -1.1112e-01,  4.1840e-01,  2.8466e-01, -7.5332e-01,\n",
      "          5.1434e-02,  3.5610e-01, -3.0344e-01,  1.6228e-01, -3.2164e-01,\n",
      "          6.0136e-01,  3.8981e-01, -2.2500e-01, -7.0480e-01, -1.3474e-01,\n",
      "          2.7103e-01, -3.1346e-01, -4.5264e-01,  1.9649e-01, -4.1346e-01,\n",
      "          1.6964e-01, -1.1299e-01,  3.4204e-01,  3.1459e-01, -2.0175e-01,\n",
      "          6.7726e-01, -4.4773e-01,  2.2404e-02, -2.1074e-02,  2.0078e-01,\n",
      "         -5.7076e-01,  6.8707e-02, -9.1237e-02, -1.8992e-02,  2.0803e-01,\n",
      "         -1.6318e-02, -4.8640e-01, -1.0765e-01,  2.7759e-01, -3.6682e-02,\n",
      "          2.8288e-01,  4.0859e-03,  2.8466e-02, -2.6598e-01,  2.7261e-02,\n",
      "          6.2972e-02, -2.1774e-01, -1.6726e-01, -2.9674e-01,  3.8176e-03,\n",
      "         -5.3871e-02,  1.2680e-01, -5.0698e-02,  3.9245e-01,  9.9389e-02,\n",
      "          5.6473e-01,  6.8827e-01,  4.6386e-02, -6.0621e-02, -1.0822e-01,\n",
      "          4.5434e-01,  1.6478e-01, -7.1782e+00, -3.4970e-01,  4.5310e-02,\n",
      "         -3.1070e-02, -2.6883e-01, -2.5365e-01, -2.9237e-01, -7.2191e-01,\n",
      "         -1.9729e-01, -2.4119e-02,  3.2511e-01, -6.4504e-02, -4.6514e-01,\n",
      "         -1.5057e-01, -3.0321e-03,  2.4606e-02]])\n"
     ]
    }
   ],
   "source": [
    "# BERT\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Function to convert tokens back to string\n",
    "def tokens_to_string(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Assuming you have your preprocessed data in processed_data\n",
    "for tokens in processed_data:\n",
    "    text = tokens_to_string(tokens)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Encoding the text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    # Get embeddings from BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the embeddings for the [CLS] token\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.22104812e-02  3.90439481e-02  6.34335354e-03  9.53142438e-03\n",
      "  -3.43552255e-03 -5.94012626e-02  5.88293187e-03 -2.69118380e-02\n",
      "  -1.90554757e-03 -2.55809408e-02 -2.82144453e-02  4.21938449e-02\n",
      "  -1.21377734e-02  2.96761356e-02  5.66073656e-02  7.18065202e-02\n",
      "   8.29539355e-03 -5.61062433e-02  3.21218595e-02 -2.16934104e-02\n",
      "   2.98571456e-02  4.57499623e-02  3.66484486e-02 -3.93893868e-02\n",
      "  -3.98137532e-02 -2.53301132e-02  6.96491543e-03  5.64314425e-02\n",
      "  -2.51339134e-02  5.26237208e-03  2.23284960e-02  4.52725403e-02\n",
      "  -2.12371051e-02  6.86998814e-02 -5.28390966e-02 -3.86430398e-02\n",
      "   6.96342788e-04 -1.33246072e-02 -6.85782361e-05 -3.45795304e-02\n",
      "  -1.72064523e-03 -4.65044342e-02  1.36138853e-02 -6.78495411e-03\n",
      "   8.92007947e-02  3.84858996e-02 -3.83521356e-02 -7.44843706e-02\n",
      "  -2.98544904e-03 -2.62982305e-02 -8.28009695e-02 -9.89479423e-02\n",
      "  -6.65626824e-02  3.30890389e-03  1.07534409e-01 -4.89039533e-02\n",
      "   3.78467403e-02 -4.80679348e-02  6.90913722e-02 -1.91480294e-03\n",
      "  -2.88868733e-02  2.56899204e-02 -8.62522051e-02  7.23137632e-02\n",
      "   1.73498280e-02 -8.33293572e-02 -3.14910300e-02 -8.34243819e-02\n",
      "   1.25197461e-02  9.56436060e-03 -6.69022948e-02 -3.53994258e-02\n",
      "  -7.70660043e-02 -2.71826442e-02  1.22600980e-03 -2.49279989e-03\n",
      "   6.68952093e-02 -1.62718501e-02  5.26595041e-02  9.40264091e-02\n",
      "  -9.66506079e-02 -1.60899665e-02 -3.39247845e-02  3.24021117e-03\n",
      "  -6.11509494e-02 -4.09853905e-02  7.13021681e-02 -1.41180158e-01\n",
      "   6.78871933e-04  1.27438791e-02 -5.64586706e-02 -1.84901841e-02\n",
      "   4.34439406e-02  1.10634752e-02 -1.66088864e-02 -4.12625372e-02\n",
      "   5.14399409e-02 -7.66576380e-02 -7.90946335e-02  8.50751325e-02\n",
      "   3.74772698e-02  5.74924536e-02 -7.77116194e-02  2.06265915e-02\n",
      "  -3.54875694e-03 -5.22658825e-02 -2.69531682e-02  7.25118965e-02\n",
      "  -2.73735859e-02 -2.45436952e-02  2.64287442e-02 -1.39022209e-02\n",
      "  -1.93441994e-02  4.10416685e-02  2.16183178e-02 -4.90593314e-02\n",
      "   3.54347727e-03  4.34715003e-02  6.71490803e-02  1.70622580e-02\n",
      "  -3.70668843e-02  2.40249913e-02 -2.72217169e-02 -2.22598836e-02\n",
      "  -1.00876167e-02 -1.32149398e-01  6.27977923e-02 -3.94805463e-33\n",
      "   7.40969703e-02  3.41811031e-02  5.90841137e-02 -8.70137417e-04\n",
      "   1.25315096e-02  2.73854434e-02  6.79102261e-03 -1.81354005e-02\n",
      "   2.19726730e-02 -1.05156805e-02  7.63802454e-02  5.19450605e-02\n",
      "  -1.12097964e-01 -1.40206161e-04 -1.59663260e-02  2.23289840e-02\n",
      "   3.91353369e-02 -6.31629117e-03  1.20562864e-02 -3.23431636e-03\n",
      "  -5.93849458e-02 -1.33147417e-02 -4.11671493e-03 -7.74123240e-04\n",
      "  -2.23163627e-02 -4.02609408e-02  1.14976518e-01 -2.69606411e-02\n",
      "   3.67419310e-02  7.79409111e-02 -2.29202816e-03  2.08259169e-02\n",
      "  -3.40651721e-02 -3.25443558e-02  4.59522195e-02  1.11710641e-03\n",
      "  -2.63744388e-02 -4.02246341e-02  4.69991714e-02  1.33734830e-02\n",
      "  -5.55743352e-02 -2.94113904e-02 -9.34452098e-03  2.21827570e-02\n",
      "   3.57763693e-02  3.43203843e-02  3.47506255e-02  7.50341788e-02\n",
      "  -9.26457439e-03  1.66143179e-02 -4.53825481e-02 -3.84881720e-02\n",
      "  -5.51229157e-02  3.78936119e-02 -3.15621495e-02 -7.18903393e-02\n",
      "   5.88884801e-02 -5.34927137e-02 -3.28661129e-02  2.18177233e-02\n",
      "   5.40095419e-02  4.59958287e-03 -9.88975354e-03  3.49558890e-02\n",
      "  -1.05345704e-01 -6.54628798e-02  2.38163322e-02 -3.12698334e-02\n",
      "  -3.25591229e-02  1.33351684e-02 -2.90595274e-02  3.93687561e-02\n",
      "   3.75336297e-02 -3.31612397e-03 -5.75901866e-02 -5.49155511e-02\n",
      "  -4.57396805e-02  4.07577902e-02  5.43595701e-02  1.50333494e-02\n",
      "   3.41504589e-02  5.43859378e-02 -2.47290954e-02 -1.55024342e-02\n",
      "   6.66411072e-02 -9.35410708e-03 -2.15121638e-02 -7.70118535e-02\n",
      "  -2.07180064e-02  5.55217899e-02 -1.48619696e-01 -2.88529824e-02\n",
      "  -7.42198899e-02  9.81310904e-02 -1.17804958e-02  1.84362477e-33\n",
      "   4.03608941e-02  4.53081168e-02 -5.51759079e-02  1.10067725e-01\n",
      "   4.86790296e-03 -2.03033946e-02 -5.50652556e-02  1.39936388e-01\n",
      "  -4.89963442e-02  6.59232736e-02  8.00750277e-04 -1.34050073e-02\n",
      "   4.93488424e-02  2.87287068e-02  2.02561542e-02 -1.60256005e-03\n",
      "   1.21373475e-01  7.47232586e-02 -4.91295718e-02 -9.02661029e-03\n",
      "  -1.01670273e-01  1.35316523e-02  3.35268825e-02 -1.37672890e-02\n",
      "  -2.30752826e-02  1.60268415e-02 -5.65961981e-03  1.26183331e-02\n",
      "  -1.13931693e-01 -3.66748795e-02  1.02019183e-01 -4.38995995e-02\n",
      "  -1.76949233e-01  6.96594361e-04  5.82083240e-02  9.18656886e-02\n",
      "   4.78363559e-02  3.73050198e-03  4.35609855e-02  2.12941058e-02\n",
      "  -1.03662582e-02  5.36237024e-02  8.70880950e-03  1.87582493e-01\n",
      "   9.37301293e-02 -1.01680577e-01  6.49460629e-02 -2.11455747e-02\n",
      "   6.20549209e-02  1.01239845e-01 -8.36601555e-02 -1.98152270e-02\n",
      "   3.39127593e-02 -1.31215397e-02 -3.86324786e-02  2.18285024e-02\n",
      "   5.55508733e-02 -4.15185504e-02  1.95742268e-02 -5.15958257e-02\n",
      "  -5.55716641e-03  5.32584898e-02 -4.89476621e-02  1.66391954e-02\n",
      "   8.41794815e-03  8.72906372e-02  1.16707897e-02 -9.59119126e-02\n",
      "  -6.66804984e-02  2.05505453e-02  3.94285247e-02 -7.88854957e-02\n",
      "   4.22426835e-02  2.89602373e-02  2.90588196e-03 -2.74734944e-02\n",
      "   3.71895917e-02 -8.32422264e-03  3.14828753e-02 -7.17919767e-02\n",
      "   4.29108329e-02 -1.53160058e-02  2.36865841e-02  3.42322253e-02\n",
      "  -3.38090174e-02 -8.77812207e-02  8.23613927e-02 -1.71041191e-02\n",
      "   4.10941392e-02 -1.50103532e-02  1.18836723e-02  4.83340546e-02\n",
      "   7.14859664e-02 -5.00981547e-02 -8.05983227e-03 -1.57257443e-08\n",
      "   1.70373339e-02  1.00411661e-02  1.50933210e-02  3.24552432e-02\n",
      "  -6.93223625e-03 -3.38851511e-02  4.08181548e-02 -3.77752148e-02\n",
      "   8.72292649e-03  7.40753263e-02  8.72548223e-02  3.49953063e-02\n",
      "   3.01169213e-02  3.63521017e-02  1.24323599e-01  2.66363584e-02\n",
      "   7.15865102e-03 -3.59011404e-02 -1.40807182e-02 -2.00014319e-02\n",
      "  -2.05676202e-02 -1.19867641e-03  2.77007222e-02 -9.87553373e-02\n",
      "   2.55832803e-02  2.69192494e-02 -9.76221338e-02  4.44229413e-03\n",
      "   1.81664191e-02  5.56191914e-02  7.98511319e-03  6.07643835e-02\n",
      "  -4.89270240e-02 -4.37583541e-03 -1.71265588e-03  2.52717845e-02\n",
      "   1.28400919e-03 -6.08062325e-03  4.17743921e-02  5.34308553e-02\n",
      "  -8.07663426e-02 -9.46289971e-02 -2.94685718e-02  1.99299529e-02\n",
      "   3.74920107e-02 -1.02705704e-02 -6.47987798e-03 -4.84594889e-02\n",
      "   1.54054426e-02 -1.01588257e-01 -8.31399560e-02  9.33370274e-03\n",
      "   1.56858861e-02  1.04230389e-01 -9.92775895e-03  2.35347860e-02\n",
      "   4.29260209e-02 -2.78645437e-02  1.56874806e-02 -1.33548137e-02\n",
      "   8.76518488e-02  9.37021822e-02 -6.54011443e-02 -4.00927179e-02]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "embeddings = model.encode(processed_data)\n",
    "\n",
    "print(embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
